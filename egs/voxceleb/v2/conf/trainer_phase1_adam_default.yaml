optim:
  opt_type: adam
  lr: 0.05
  amsgrad: true
  beta1: 0.9
  beta2: 0.95
  weight_decay: 4e-4
lrsched:
  lrsch_type: exp_lr
  decay_steps: 8000
  hold_steps: 40000
  min_lr: 1.0e-05
  decay_rate: 0.5
  warmup_steps: 1000
  update_lr_on_opt_step: true
use_amp: true
log_interval: 1000
epochs: 30
eff_batch_size: 1024
train_mode: hf-feats-frozen-nograd
